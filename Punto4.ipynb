{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion inicial\n",
    "%pip install -q opendatasets keras-tuner\n",
    "!ls\n",
    "!rm -rf output\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import keras_tuner as kt\n",
    "import opendatasets as od\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout, Flatten,Dense, Activation, BatchNormalization)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bdf75",
   "metadata": {},
   "source": [
    "## Cargue DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/yusufemir/lemon-quality-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f55e88",
   "metadata": {},
   "source": [
    "### Funci√≥n para Cargar Im√°genes del Dataset\n",
    "\n",
    "Esta funci√≥n permite cargar im√°genes manualmente usando OpenCV, √∫til para tener control total sobre el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_dataset(dataset_path, clases, img_size=(75, 75), test_size=0.15, val_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Carga im√°genes del dataset usando OpenCV con divisi√≥n train/val/test.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CARGANDO IM√ÅGENES CON OPENCV\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    label_to_idx = {clase: idx for idx, clase in enumerate(clases)}\n",
    "    \n",
    "    stats = {\n",
    "        'total_loaded': 0,\n",
    "        'total_skipped': 0,\n",
    "        'by_class': {}\n",
    "    }\n",
    "    \n",
    "    # Cargar im√°genes por clase\n",
    "    for clase in clases:\n",
    "        clase_path = Path(dataset_path) / clase\n",
    "        \n",
    "        if not clase_path.exists():\n",
    "            print(f\" Advertencia: La carpeta '{clase}' no existe en {dataset_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Obtener todas las im√°genes\n",
    "        img_files = list(clase_path.glob('*.jpg')) + list(clase_path.glob('*.png'))\n",
    "        loaded_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(f\"\\n Procesando clase '{clase}': {len(img_files)} archivos encontrados\")\n",
    "        \n",
    "        for img_path in tqdm(img_files, desc=f\"  Cargando {clase}\"):\n",
    "            try:\n",
    "                # Leer imagen con OpenCV\n",
    "                img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "                \n",
    "                if img is None:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Convertir BGR a RGB (OpenCV usa BGR por defecto)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Redimensionar a tama√±o objetivo\n",
    "                img = cv2.resize(img, img_size)\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(label_to_idx[clase])\n",
    "                loaded_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        stats['by_class'][clase] = {\n",
    "            'loaded': loaded_count,\n",
    "            'skipped': skipped_count\n",
    "        }\n",
    "        stats['total_loaded'] += loaded_count\n",
    "        stats['total_skipped'] += skipped_count\n",
    "        \n",
    "        print(f\" Cargadas: {loaded_count} | Omitidas: {skipped_count}\")\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    X = np.array(images, dtype=np.uint8)\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESUMEN DE CARGA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total im√°genes cargadas: {stats['total_loaded']}\")\n",
    "    print(f\"Total im√°genes omitidas: {stats['total_skipped']}\")\n",
    "    print(f\"Forma del array X: {X.shape}\")\n",
    "    print(f\"Forma del array y: {y.shape}\")\n",
    "    \n",
    "    # Dividir en train/val/test\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DIVISI√ìN DE DATOS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Primero separar test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Luego separar train y val del resto\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Ajustar proporci√≥n\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]} im√°genes ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Val:   {X_val.shape[0]} im√°genes ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test:  {X_test.shape[0]} im√°genes ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'class_names': clases,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "print(\"‚úì Funci√≥n 'load_images_from_dataset' definida correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95888c26",
   "metadata": {},
   "source": [
    "#### Ejemplo de Uso (Opcional)\n",
    "\n",
    "Si deseas cargar las im√°genes manualmente con OpenCV en lugar de usar `ImageDataGenerator`, descomenta y ejecuta la siguiente celda.\n",
    "\n",
    "**Nota:** Por defecto usaremos `ImageDataGenerator` que ya est√° implementado m√°s adelante, pero esta funci√≥n est√° disponible si necesitas mayor control sobre la carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos con OpenCV (cambia img_size si es necesario)\n",
    "data = load_images_from_dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    clases=clases,\n",
    "    img_size=(75, 75),  # O (300, 300) seg√∫n prefieras\n",
    "    test_size=0.15,\n",
    "    val_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Acceder a los datos\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']\n",
    "\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Clases: {data['class_names']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242cca6",
   "metadata": {},
   "source": [
    "## 4.1 An√°lisis del Dataset\n",
    "\n",
    "En esta secci√≥n se realizar√° el an√°lisis completo del dataset de limones, incluyendo:\n",
    "- Distribuci√≥n de clases\n",
    "- Dimensiones de las im√°genes\n",
    "- Estad√≠sticas de p√≠xeles (RGB)\n",
    "- Ratio de desbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del dataset\n",
    "dataset_path = Path('/content/lemon-quality-dataset/lemon_dataset')\n",
    "clases = ['bad_quality', 'good_quality', 'empty_background']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AN√ÅLISIS DEL DATASET - LEMON CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Ruta del dataset: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e3849",
   "metadata": {},
   "source": [
    "### 1. Distribuci√≥n de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar n√∫mero de im√°genes por clase\n",
    "distribucion_clases = {}\n",
    "for clase in clases:\n",
    "    ruta_clase = dataset_path / clase\n",
    "    num_imagenes = len(list(ruta_clase.glob('*.jpg')) + list(ruta_clase.glob('*.png')))\n",
    "    distribucion_clases[clase] = num_imagenes\n",
    "    print(f\"{clase}: {num_imagenes} im√°genes\")\n",
    "\n",
    "print(f\"\\nTotal de im√°genes: {sum(distribucion_clases.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fe2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de barras - Distribuci√≥n de clases\n",
    "plt.figure(figsize=(10, 6))\n",
    "colores = ['#FF6B6B', '#4ECDC4']\n",
    "barras = plt.bar(distribucion_clases.keys(), distribucion_clases.values(), \n",
    "                 color=colores, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for barra in barras:\n",
    "    altura = barra.get_height()\n",
    "    plt.text(barra.get_x() + barra.get_width()/2., altura,\n",
    "             f'{int(altura)}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Distribuci√≥n de Clases en el Dataset de Limones', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Clase', fontsize=12)\n",
    "plt.ylabel('N√∫mero de Muestras', fontsize=12)\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8b267",
   "metadata": {},
   "source": [
    "### 2. Dimensiones de las Im√°genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e80dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar dimensiones de las im√°genes\n",
    "dimensiones = []\n",
    "for clase in clases:\n",
    "    ruta_clase = dataset_path / clase\n",
    "    imagenes = list(ruta_clase.glob('*.jpg')) + list(ruta_clase.glob('*.png'))\n",
    "    \n",
    "    for img_path in imagenes[:20]:  # Revisar primeras 20 de cada clase\n",
    "        img = Image.open(img_path)\n",
    "        dimensiones.append((img.width, img.height, len(img.getbands())))\n",
    "\n",
    "# Encontrar dimensiones √∫nicas\n",
    "dimensiones_unicas = list(set(dimensiones))\n",
    "print(\"Dimensiones encontradas en el dataset:\")\n",
    "for dim in dimensiones_unicas:\n",
    "    count = dimensiones.count(dim)\n",
    "    print(f\"  - {dim[0]} √ó {dim[1]} √ó {dim[2]} (Ancho √ó Alto √ó Canales): {count} im√°genes\")\n",
    "\n",
    "# Determinar dimensi√≥n m√°s com√∫n\n",
    "dim_comun = max(set(dimensiones), key=dimensiones.count)\n",
    "print(f\"\\n‚úì Dimensi√≥n m√°s com√∫n: {dim_comun[0]} √ó {dim_comun[1]} √ó {dim_comun[2]} (Ancho √ó Alto √ó Canales)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58b78b",
   "metadata": {},
   "source": [
    "### 3. Estad√≠sticas de P√≠xeles (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b097bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular estad√≠sticas de p√≠xeles por canal RGB\n",
    "pixeles_r, pixeles_g, pixeles_b = [], [], []\n",
    "num_muestras = 50  # Muestras por clase para an√°lisis\n",
    "\n",
    "print(f\"Recolectando {num_muestras} muestras por clase...\\n\")\n",
    "\n",
    "for clase in clases:\n",
    "    ruta_clase = dataset_path / clase\n",
    "    imagenes = list(ruta_clase.glob('*.jpg')) + list(ruta_clase.glob('*.png'))\n",
    "    print(f\"  ‚úì Procesando {clase}: {len(imagenes)} im√°genes encontradas (usando {min(num_muestras, len(imagenes))})\")\n",
    "    \n",
    "    for img_path in imagenes[:num_muestras]:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Muestreo aleatorio de p√≠xeles (para eficiencia)\n",
    "        muestra_indices = np.random.choice(img_array.shape[0] * img_array.shape[1], \n",
    "                                          size=min(1000, img_array.shape[0] * img_array.shape[1]), \n",
    "                                          replace=False)\n",
    "        pixeles_planos = img_array.reshape(-1, 3)[muestra_indices]\n",
    "        \n",
    "        pixeles_r.extend(pixeles_planos[:, 0])\n",
    "        pixeles_g.extend(pixeles_planos[:, 1])\n",
    "        pixeles_b.extend(pixeles_planos[:, 2])\n",
    "\n",
    "# Convertir a arrays numpy\n",
    "pixeles_r = np.array(pixeles_r)\n",
    "pixeles_g = np.array(pixeles_g)\n",
    "pixeles_b = np.array(pixeles_b)\n",
    "\n",
    "# Calcular y mostrar estad√≠sticas GLOBALES (todas las clases combinadas)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESTAD√çSTICAS GLOBALES DE P√çXELES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de p√≠xeles analizados por canal: {len(pixeles_r):,}\")\n",
    "print(f\"Muestras por clase: {num_muestras} im√°genes √ó 2 clases = {num_muestras * 3} im√°genes\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "estadisticas = {\n",
    "    'Canal Rojo (R)': {'media': np.mean(pixeles_r), 'std': np.std(pixeles_r)},\n",
    "    'Canal Verde (G)': {'media': np.mean(pixeles_g), 'std': np.std(pixeles_g)},\n",
    "    'Canal Azul (B)': {'media': np.mean(pixeles_b), 'std': np.std(pixeles_b)}\n",
    "}\n",
    "\n",
    "for canal, stats in estadisticas.items():\n",
    "    print(f\"{canal}:\")\n",
    "    print(f\"  Media: {stats['media']:.2f}\")\n",
    "    print(f\"  Desviaci√≥n Est√°ndar: {stats['std']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de distribuci√≥n de p√≠xeles por canal\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "colores_canales = ['red', 'green', 'blue']\n",
    "pixeles_canales = [pixeles_r, pixeles_g, pixeles_b]\n",
    "titulos = ['Canal Rojo (R)', 'Canal Verde (G)', 'Canal Azul (B)']\n",
    "\n",
    "for ax, pixeles, color, titulo in zip(axes, pixeles_canales, colores_canales, titulos):\n",
    "    ax.hist(pixeles, bins=50, color=color, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(titulo, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('Valor de P√≠xel (0-255)', fontsize=10)\n",
    "    ax.set_ylabel('Frecuencia', fontsize=10)\n",
    "    ax.axvline(np.mean(pixeles), color='black', linestyle='--', linewidth=2, \n",
    "               label=f'Media: {np.mean(pixeles):.1f}')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribuci√≥n de Valores de P√≠xeles por Canal RGB', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd209c",
   "metadata": {},
   "source": [
    "### 4. Ratio de Desbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11840a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular ratio de desbalance\n",
    "clase_mayor = max(distribucion_clases.values())\n",
    "clase_menor = min(distribucion_clases.values())\n",
    "ratio_desbalance = clase_mayor / clase_menor\n",
    "\n",
    "print(\"An√°lisis de Desbalance del Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Clase con m√°s muestras: {clase_mayor}\")\n",
    "print(f\"Clase con menos muestras: {clase_menor}\")\n",
    "print(f\"Ratio de desbalance: {ratio_desbalance:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09ad52",
   "metadata": {},
   "source": [
    "### Resumen del An√°lisis del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final del an√°lisis\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN DEL AN√ÅLISIS DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de im√°genes: {sum(distribucion_clases.values())}\")\n",
    "print(f\"N√∫mero de clases: {len(clases)}\")\n",
    "print(f\" Dimensi√≥n com√∫n: {dim_comun[0]} √ó {dim_comun[1]} √ó {dim_comun[2]} (Ancho √ó Alto √ó Canales)\")\n",
    "print(f\"Ratio de desbalance: {ratio_desbalance:.2f}\", end=\" \")\n",
    "print(\"(DESBALANCEADO )\" if ratio_desbalance > 1.5 else \"(BALANCEADO ‚úì)\")\n",
    "print()\n",
    "print(\"Estad√≠sticas RGB:\")\n",
    "for canal, stats in estadisticas.items():\n",
    "    print(f\"  {canal}: Œº={stats['media']:.2f}, œÉ={stats['std']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed395fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e805ebd4",
   "metadata": {},
   "source": [
    "## 4.2 Baseline y Arquitecturas\n",
    "\n",
    "En esta secci√≥n implementaremos:\n",
    "- **Fase 1**: Baseline MLP (red neuronal sin convoluciones)\n",
    "- **Fase 2**: 3 arquitecturas CNN con complejidad progresiva\n",
    "\n",
    "**T√©cnicas implementadas:**\n",
    "- Batch Normalization\n",
    "- Data Augmentation\n",
    "- Dropout\n",
    "- Early Stopping\n",
    "- Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712631",
   "metadata": {},
   "source": [
    "### Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b3d5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de par√°metros\n",
    "# Para pruebas r√°pidas usamos 75x75, luego puedes cambiar a 300x300\n",
    "IMG_HEIGHT = 75\n",
    "IMG_WIDTH = 75\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "SEED = 42\n",
    "FAST_RUN = True\n",
    "\n",
    "if FAST_RUN: \n",
    "    EPOCHS = 5\n",
    "    print('‚ö° Modo r√°pido activado: 5 √©pocas con im√°genes 75x75') \n",
    "else: \n",
    "    print('üîÑ Modo normal: 20 √©pocas')\n",
    "\n",
    "# Establecer semilla para reproducibilidad\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURACI√ìN DE PAR√ÅMETROS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dimensiones de imagen: {IMG_HEIGHT}x{IMG_WIDTH} (optimizado para pruebas)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"√âpocas: {EPOCHS}\")\n",
    "print(f\"Clases: {clases}\")\n",
    "print(f\"N√∫mero de clases: {len(clases)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01d918a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos usando la funci√≥n OpenCV (m√°s r√°pido y eficiente para pruebas)\n",
    "# Divisi√≥n: 70% train, 15% validation, 15% test\n",
    "\n",
    "print(\"\\nüîÑ Cargando datos con OpenCV...\")\n",
    "data = load_images_from_dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    clases=clases,\n",
    "    img_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    test_size=0.15,\n",
    "    val_size=0.15,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Extraer los datos\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Convertir etiquetas a categorical (one-hot encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes=len(clases))\n",
    "y_val_cat = to_categorical(y_val, num_classes=len(clases))\n",
    "y_test_cat = to_categorical(y_test, num_classes=len(clases))\n",
    "\n",
    "# Normalizar los datos (convertir de [0, 255] a [0, 1])\n",
    "print(\"\\nüìä Normalizando datos...\")\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "print(\"‚úÖ Datos normalizados a rango [0, 1]\")\n",
    "\n",
    "# Aplicar Data Augmentation al conjunto de entrenamiento\n",
    "print(\"\\nüé® Aplicando Data Augmentation al conjunto de entrenamiento...\")\n",
    "\n",
    "datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Ajustar el generador con los datos de entrenamiento\n",
    "datagen_augmented.fit(X_train)\n",
    "\n",
    "print(\" Data Augmentation configurado\")\n",
    "print(\"\\nTransformaciones aplicadas:\")\n",
    "print(\"  ‚Ä¢ Rotaci√≥n: ¬±20¬∞\")\n",
    "print(\"  ‚Ä¢ Desplazamiento horizontal/vertical: ¬±20%\")\n",
    "print(\"  ‚Ä¢ Flip horizontal: S√≠\")\n",
    "print(\"  ‚Ä¢ Zoom: ¬±15%\")\n",
    "print(\"  ‚Ä¢ Shear: ¬±15%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d13f3",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares para Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43e63244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para graficar historial de entrenamiento\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Grafica loss y accuracy durante el entrenamiento\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_title(f'{model_name} - Loss', fontweight='bold', fontsize=13)\n",
    "    axes[0].set_xlabel('√âpoca', fontsize=11)\n",
    "    axes[0].set_ylabel('Loss', fontsize=11)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    axes[1].set_title(f'{model_name} - Accuracy', fontweight='bold', fontsize=13)\n",
    "    axes[1].set_xlabel('√âpoca', fontsize=11)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Funci√≥n para crear matriz de confusi√≥n\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Crea y muestra matriz de confusi√≥n\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Frecuencia'})\n",
    "    plt.title(f'Matriz de Confusi√≥n - {model_name}', fontweight='bold', fontsize=14, pad=20)\n",
    "    plt.ylabel('Clase Real', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Clase Predicha', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Funci√≥n para evaluar modelo (ahora usa arrays numpy en lugar de generadores)\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo en train, val y test\n",
    "    Retorna diccionario con m√©tricas\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUACI√ìN: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Evaluar en cada conjunto\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Calcular overfitting\n",
    "    overfitting = train_acc - val_acc\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Accuracy: {val_acc*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Overfitting (Train - Val): {overfitting*100:.2f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'train_acc': train_acc * 100,\n",
    "        'val_acc': val_acc * 100,\n",
    "        'test_acc': test_acc * 100,\n",
    "        'overfitting': overfitting * 100,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'test_loss': test_loss\n",
    "    }\n",
    "\n",
    "print(\"‚úì Funciones auxiliares definidas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b56b9",
   "metadata": {},
   "source": [
    "---\n",
    "## FASE 1: Baseline MLP (0.2 pts)\n",
    "\n",
    "Red neuronal sin convoluciones:\n",
    "- **Arquitectura**: Input ‚Üí Flatten ‚Üí Dense(128, relu) ‚Üí Dense(64, relu) ‚Üí Dense(num_clases, softmax)\n",
    "- **√âpocas**: 20\n",
    "- **Objetivo**: Establecer baseline para comparaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d36cca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcci√≥n del modelo Baseline MLP\n",
    "def build_baseline_mlp(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Modelo baseline sin convoluciones\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', name='dense_1'),\n",
    "        layers.Dense(64, activation='relu', name='dense_2'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='Baseline_MLP')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "baseline_mlp = build_baseline_mlp(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    num_classes=len(clases)\n",
    ")\n",
    "\n",
    "# Compilar modelo\n",
    "baseline_mlp.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODELO 0: BASELINE MLP\")\n",
    "print(f\"{'='*60}\")\n",
    "baseline_mlp.summary()\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cf31374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Baseline MLP\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENTRENANDO BASELINE MLP\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar SIN data augmentation (es un MLP, no se beneficia de aug)\n",
    "history_mlp = baseline_mlp.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "time_per_epoch = total_time / EPOCHS\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENTRENAMIENTO COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Tiempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "print(f\"Tiempo por √©poca: {time_per_epoch:.2f} segundos\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2bbb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Baseline MLP\n",
    "metrics_mlp = evaluate_model(\n",
    "    baseline_mlp, \n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    X_test, y_test_cat,\n",
    "    \"Baseline MLP\"\n",
    ")\n",
    "\n",
    "# Graficar historial de entrenamiento\n",
    "plot_training_history(history_mlp, \"Baseline MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48c45324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n para Baseline MLP\n",
    "y_pred_mlp = baseline_mlp.predict(X_test, verbose=0)\n",
    "y_pred_classes_mlp = np.argmax(y_pred_mlp, axis=1)\n",
    "y_true_mlp = y_test\n",
    "\n",
    "cm_mlp = plot_confusion_matrix(\n",
    "    y_true_mlp, \n",
    "    y_pred_classes_mlp, \n",
    "    clases,\n",
    "    \"Baseline MLP\"\n",
    ")\n",
    "\n",
    "# Guardar m√©tricas del baseline\n",
    "results_table = {\n",
    "    'Exp': [0],\n",
    "    'Arquitectura': ['MLP-Baseline'],\n",
    "    'Train_Acc': [f\"{metrics_mlp['train_acc']:.2f}\"],\n",
    "    'Val_Acc': [f\"{metrics_mlp['val_acc']:.2f}\"],\n",
    "    'Test_Acc': [f\"{metrics_mlp['test_acc']:.2f}\"],\n",
    "    'Params': [baseline_mlp.count_params()],\n",
    "    'Time_per_Epoch': [f\"{time_per_epoch:.2f}\"],\n",
    "    'Overfitting': [f\"{metrics_mlp['overfitting']:.2f}\"]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN BASELINE MLP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train Accuracy: {metrics_mlp['train_acc']:.2f}%\")\n",
    "print(f\"Val Accuracy: {metrics_mlp['val_acc']:.2f}%\")\n",
    "print(f\"Test Accuracy: {metrics_mlp['test_acc']:.2f}%\")\n",
    "print(f\"Par√°metros: {baseline_mlp.count_params():,}\")\n",
    "print(f\"Tiempo/√âpoca: {time_per_epoch:.2f} seg\")\n",
    "print(f\"Overfitting: {metrics_mlp['overfitting']:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423448b",
   "metadata": {},
   "source": [
    "---\n",
    "## FASE 2: Arquitecturas CNN (0.7 pts)\n",
    "\n",
    "Implementaremos 3 arquitecturas CNN con complejidad progresiva:\n",
    "\n",
    "### **CNN 1**: Simple (Conv32 ‚Üí Conv64 ‚Üí Dense)\n",
    "- 2 capas convolucionales\n",
    "- Sin t√©cnicas avanzadas\n",
    "\n",
    "### **CNN 2**: Intermedia con Batch Normalization\n",
    "- 3 capas convolucionales\n",
    "- Batch Normalization\n",
    "- Dropout\n",
    "\n",
    "### **CNN 3**: Avanzada con Early Stopping y LR Scheduling\n",
    "- 4 capas convolucionales\n",
    "- Batch Normalization\n",
    "- Dropout progresivo\n",
    "- Early Stopping\n",
    "- Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222904c8",
   "metadata": {},
   "source": [
    "### CNN 1: Arquitectura Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ea362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 1: Arquitectura Simple\n",
    "def build_cnn1(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    CNN Simple: Conv(32) ‚Üí Conv(64) ‚Üí MaxPool ‚Üí Flatten ‚Üí Dense(128) ‚Üí Dense(64) ‚Üí Output\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Bloque Convolucional 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Bloque Convolucional 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Capas Densas\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(128, activation='relu', name='dense1'),\n",
    "        layers.Dense(64, activation='relu', name='dense2'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='CNN_Simple')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo CNN1\n",
    "cnn1 = build_cnn1(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    num_classes=len(clases)\n",
    ")\n",
    "\n",
    "# Compilar\n",
    "cnn1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODELO 1: CNN SIMPLE\")\n",
    "print(f\"{'='*60}\")\n",
    "cnn1.summary()\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar CNN1 con aumento de datos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENTRENANDO CNN 1: SIMPLE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time_cnn1 = time.time()\n",
    "\n",
    "# Crear generador de aumento de datos para entrenamiento\n",
    "train_generator_cnn1 = datagen_augmented.flow(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "history_cnn1 = cnn1.fit(\n",
    "    train_generator_cnn1,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time_cnn1 = time.time()\n",
    "total_time_cnn1 = end_time_cnn1 - start_time_cnn1\n",
    "time_per_epoch_cnn1 = total_time_cnn1 / EPOCHS\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENTRENAMIENTO CNN1 COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Tiempo total: {total_time_cnn1:.2f} segundos ({total_time_cnn1/60:.2f} minutos)\")\n",
    "print(f\"Tiempo por √©poca: {time_per_epoch_cnn1:.2f} segundos\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ab465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar CNN1\n",
    "metrics_cnn1 = evaluate_model(\n",
    "    cnn1, \n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    X_test, y_test_cat,\n",
    "    \"CNN1 Simple\"\n",
    ")\n",
    "plot_training_history(history_cnn1, \"CNN1 Simple\")\n",
    "\n",
    "# Matriz de confusi√≥n CNN1\n",
    "y_pred_cnn1 = cnn1.predict(X_test, verbose=0)\n",
    "y_pred_classes_cnn1 = np.argmax(y_pred_cnn1, axis=1)\n",
    "y_true_cnn1 = y_test\n",
    "\n",
    "cm_cnn1 = plot_confusion_matrix(y_true_cnn1, y_pred_classes_cnn1, clases, \"CNN1 Simple\")\n",
    "\n",
    "# Agregar a tabla de resultados\n",
    "results_table['Exp'].append(1)\n",
    "results_table['Arquitectura'].append('Conv32‚ÜíConv64‚ÜíMaxPool‚ÜíDense128‚ÜíDense64')\n",
    "results_table['Train_Acc'].append(f\"{metrics_cnn1['train_acc']:.2f}\")\n",
    "results_table['Val_Acc'].append(f\"{metrics_cnn1['val_acc']:.2f}\")\n",
    "results_table['Test_Acc'].append(f\"{metrics_cnn1['test_acc']:.2f}\")\n",
    "results_table['Params'].append(cnn1.count_params())\n",
    "results_table['Time_per_Epoch'].append(f\"{time_per_epoch_cnn1:.2f}\")\n",
    "results_table['Overfitting'].append(f\"{metrics_cnn1['overfitting']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef65508",
   "metadata": {},
   "source": [
    "### CNN 2: Arquitectura Intermedia con Batch Normalization y Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 2: Arquitectura Intermedia con Batch Normalization y Dropout\n",
    "def build_cnn2(input_shape, num_classes, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    CNN Intermedia con Batch Normalization y Dropout\n",
    "    Conv32 ‚Üí BN ‚Üí Conv64 ‚Üí BN ‚Üí Conv128 ‚Üí BN ‚Üí MaxPool ‚Üí Dropout ‚Üí Dense\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Bloque Convolucional 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Bloque Convolucional 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Bloque Convolucional 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        \n",
    "        # Capas Densas\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dropout(dropout_rate, name='dropout1'),\n",
    "        layers.Dense(128, activation='relu', name='dense1'),\n",
    "        layers.Dropout(dropout_rate, name='dropout2'),\n",
    "        layers.Dense(64, activation='relu', name='dense2'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='CNN_BN_Dropout')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo CNN2\n",
    "cnn2 = build_cnn2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    num_classes=len(clases),\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Compilar\n",
    "cnn2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODELO 2: CNN CON BATCH NORMALIZATION Y DROPOUT\")\n",
    "print(f\"{'='*60}\")\n",
    "cnn2.summary()\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dropout rate: 0.3\")\n",
    "print(f\"Batch Normalization: Despu√©s de cada Conv\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962097a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar CNN2 con aumento de datos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENTRENANDO CNN 2: CON BATCH NORMALIZATION Y DROPOUT\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time_cnn2 = time.time()\n",
    "\n",
    "# Crear generador de aumento de datos para entrenamiento\n",
    "train_generator_cnn2 = datagen_augmented.flow(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "history_cnn2 = cnn2.fit(\n",
    "    train_generator_cnn2,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time_cnn2 = time.time()\n",
    "total_time_cnn2 = end_time_cnn2 - start_time_cnn2\n",
    "time_per_epoch_cnn2 = total_time_cnn2 / EPOCHS\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENTRENAMIENTO CNN2 COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Tiempo total: {total_time_cnn2:.2f} segundos ({total_time_cnn2/60:.2f} minutos)\")\n",
    "print(f\"Tiempo por √©poca: {time_per_epoch_cnn2:.2f} segundos\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37802199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar CNN2\n",
    "metrics_cnn2 = evaluate_model(\n",
    "    cnn2, \n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    X_test, y_test_cat,\n",
    "    \"CNN2 BN+Dropout\"\n",
    ")\n",
    "plot_training_history(history_cnn2, \"CNN2 BN+Dropout\")\n",
    "\n",
    "# Matriz de confusi√≥n CNN2\n",
    "y_pred_cnn2 = cnn2.predict(X_test, verbose=0)\n",
    "y_pred_classes_cnn2 = np.argmax(y_pred_cnn2, axis=1)\n",
    "y_true_cnn2 = y_test\n",
    "\n",
    "cm_cnn2 = plot_confusion_matrix(y_true_cnn2, y_pred_classes_cnn2, clases, \"CNN2 BN+Dropout\")\n",
    "\n",
    "# Agregar a tabla de resultados\n",
    "results_table['Exp'].append(2)\n",
    "results_table['Arquitectura'].append('Conv32‚ÜíConv64‚ÜíConv128 + BN + Dropout(0.3)')\n",
    "results_table['Train_Acc'].append(f\"{metrics_cnn2['train_acc']:.2f}\")\n",
    "results_table['Val_Acc'].append(f\"{metrics_cnn2['val_acc']:.2f}\")\n",
    "results_table['Test_Acc'].append(f\"{metrics_cnn2['test_acc']:.2f}\")\n",
    "results_table['Params'].append(cnn2.count_params())\n",
    "results_table['Time_per_Epoch'].append(f\"{time_per_epoch_cnn2:.2f}\")\n",
    "results_table['Overfitting'].append(f\"{metrics_cnn2['overfitting']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc76abd",
   "metadata": {},
   "source": [
    "### CNN 3: Arquitectura Avanzada con Early Stopping y LR Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 3: Arquitectura Avanzada\n",
    "def build_cnn3(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    CNN Avanzada con 4 capas convolucionales, BN y Dropout progresivo\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Bloque Convolucional 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        layers.Dropout(0.2, name='dropout1'),\n",
    "        \n",
    "        # Bloque Convolucional 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        layers.Dropout(0.3, name='dropout2'),\n",
    "        \n",
    "        # Bloque Convolucional 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        layers.Dropout(0.4, name='dropout3'),\n",
    "        \n",
    "        # Bloque Convolucional 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4'),\n",
    "        layers.BatchNormalization(name='bn4'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool4'),\n",
    "        layers.Dropout(0.5, name='dropout4'),\n",
    "        \n",
    "        # Capas Densas\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(256, activation='relu', name='dense1'),\n",
    "        layers.Dropout(0.5, name='dropout5'),\n",
    "        layers.Dense(128, activation='relu', name='dense2'),\n",
    "        layers.Dropout(0.4, name='dropout6'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='CNN_Advanced')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo CNN3\n",
    "cnn3 = build_cnn3(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    num_classes=len(clases)\n",
    ")\n",
    "\n",
    "# Compilar\n",
    "cnn3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODELO 3: CNN AVANZADA\")\n",
    "print(f\"{'='*60}\")\n",
    "cnn3.summary()\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dropout progresivo: 0.2 ‚Üí 0.3 ‚Üí 0.4 ‚Üí 0.5\")\n",
    "print(f\"Batch Normalization: Despu√©s de cada Conv\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51274685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar callbacks para CNN3\n",
    "# Early Stopping: detiene el entrenamiento si no mejora val_loss\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduling: reduce LR cuando val_loss se estanca\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_cnn3 = [early_stopping, reduce_lr]\n",
    "\n",
    "print(\"Callbacks configurados:\")\n",
    "print(f\"  ‚Ä¢ Early Stopping: patience=5, monitor=val_loss\")\n",
    "print(f\"  ‚Ä¢ ReduceLROnPlateau: factor=0.5, patience=3, min_lr=1e-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e00bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar CNN3 con callbacks y aumento de datos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENTRENANDO CNN 3: AVANZADA CON EARLY STOPPING Y LR SCHEDULING\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time_cnn3 = time.time()\n",
    "\n",
    "# Crear generador de aumento de datos para entrenamiento\n",
    "train_generator_cnn3 = datagen_augmented.flow(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "history_cnn3 = cnn3.fit(\n",
    "    train_generator_cnn3,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    callbacks=callbacks_cnn3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time_cnn3 = time.time()\n",
    "total_time_cnn3 = end_time_cnn3 - start_time_cnn3\n",
    "epochs_trained_cnn3 = len(history_cnn3.history['loss'])\n",
    "time_per_epoch_cnn3 = total_time_cnn3 / epochs_trained_cnn3\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENTRENAMIENTO CNN3 COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"√âpocas entrenadas: {epochs_trained_cnn3} (de {EPOCHS} m√°ximas)\")\n",
    "print(f\"Tiempo total: {total_time_cnn3:.2f} segundos ({total_time_cnn3/60:.2f} minutos)\")\n",
    "print(f\"Tiempo por √©poca: {time_per_epoch_cnn3:.2f} segundos\")\n",
    "if epochs_trained_cnn3 < EPOCHS:\n",
    "    print(f\"‚ö†Ô∏è  Early Stopping activado en √©poca {epochs_trained_cnn3}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar CNN3\n",
    "metrics_cnn3 = evaluate_model(\n",
    "    cnn3, \n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    X_test, y_test_cat,\n",
    "    \"CNN3 Advanced\"\n",
    ")\n",
    "plot_training_history(history_cnn3, \"CNN3 Advanced\")\n",
    "\n",
    "# Matriz de confusi√≥n CNN3\n",
    "y_pred_cnn3 = cnn3.predict(X_test, verbose=0)\n",
    "y_pred_classes_cnn3 = np.argmax(y_pred_cnn3, axis=1)\n",
    "y_true_cnn3 = y_test\n",
    "\n",
    "cm_cnn3 = plot_confusion_matrix(y_true_cnn3, y_pred_classes_cnn3, clases, \"CNN3 Advanced\")\n",
    "\n",
    "# Agregar a tabla de resultados\n",
    "results_table['Exp'].append(3)\n",
    "results_table['Arquitectura'].append('Conv32‚Üí64‚Üí128‚Üí256 + BN + Dropout + ES + LR')\n",
    "results_table['Train_Acc'].append(f\"{metrics_cnn3['train_acc']:.2f}\")\n",
    "results_table['Val_Acc'].append(f\"{metrics_cnn3['val_acc']:.2f}\")\n",
    "results_table['Test_Acc'].append(f\"{metrics_cnn3['test_acc']:.2f}\")\n",
    "results_table['Params'].append(cnn3.count_params())\n",
    "results_table['Time_per_Epoch'].append(f\"{time_per_epoch_cnn3:.2f}\")\n",
    "results_table['Overfitting'].append(f\"{metrics_cnn3['overfitting']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2c0ae",
   "metadata": {},
   "source": [
    "---\n",
    "### Tabla Comparativa de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con resultados\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results_table)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\" \"*40 + \"TABLA COMPARATIVA DE RESULTADOS\")\n",
    "print(\"=\"*120)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "print(\"\\nNotas:\")\n",
    "print(\"  ‚Ä¢ Overfitting = Train_Acc - Val_Acc\")\n",
    "print(\"  ‚Ä¢ BN = Batch Normalization\")\n",
    "print(\"  ‚Ä¢ ES = Early Stopping\")\n",
    "print(\"  ‚Ä¢ LR = Learning Rate Scheduling\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n gr√°fica de comparaci√≥n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Comparaci√≥n de Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "x = df_results['Exp']\n",
    "width = 0.25\n",
    "x_pos = np.arange(len(x))\n",
    "\n",
    "train_accs = [float(acc) for acc in df_results['Train_Acc']]\n",
    "val_accs = [float(acc) for acc in df_results['Val_Acc']]\n",
    "test_accs = [float(acc) for acc in df_results['Test_Acc']]\n",
    "\n",
    "ax1.bar(x_pos - width, train_accs, width, label='Train', color='#3498db', alpha=0.8)\n",
    "ax1.bar(x_pos, val_accs, width, label='Validation', color='#2ecc71', alpha=0.8)\n",
    "ax1.bar(x_pos + width, test_accs, width, label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Experimento', fontweight='bold', fontsize=11)\n",
    "ax1.set_ylabel('Accuracy (%)', fontweight='bold', fontsize=11)\n",
    "ax1.set_title('Comparaci√≥n de Accuracy por Modelo', fontweight='bold', fontsize=13)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f\"Exp {i}\" for i in x])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Overfitting\n",
    "ax2 = axes[0, 1]\n",
    "overfitting_vals = [float(ov) for ov in df_results['Overfitting']]\n",
    "colors_ov = ['#e74c3c' if ov > 5 else '#f39c12' if ov > 2 else '#2ecc71' for ov in overfitting_vals]\n",
    "ax2.bar(x_pos, overfitting_vals, color=colors_ov, alpha=0.8, edgecolor='black')\n",
    "ax2.axhline(y=5, color='red', linestyle='--', linewidth=2, label='Umbral Alto (5%)')\n",
    "ax2.axhline(y=2, color='orange', linestyle='--', linewidth=2, label='Umbral Medio (2%)')\n",
    "ax2.set_xlabel('Experimento', fontweight='bold', fontsize=11)\n",
    "ax2.set_ylabel('Overfitting (%)', fontweight='bold', fontsize=11)\n",
    "ax2.set_title('Nivel de Overfitting por Modelo', fontweight='bold', fontsize=13)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f\"Exp {i}\" for i in x])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. N√∫mero de Par√°metros\n",
    "ax3 = axes[1, 0]\n",
    "params = df_results['Params']\n",
    "ax3.bar(x_pos, params, color='#9b59b6', alpha=0.8, edgecolor='black')\n",
    "ax3.set_xlabel('Experimento', fontweight='bold', fontsize=11)\n",
    "ax3.set_ylabel('N√∫mero de Par√°metros', fontweight='bold', fontsize=11)\n",
    "ax3.set_title('Complejidad del Modelo (Par√°metros)', fontweight='bold', fontsize=13)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f\"Exp {i}\" for i in x])\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for i, v in enumerate(params):\n",
    "    ax3.text(i, v + max(params)*0.02, f'{v:,}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 4. Tiempo por √âpoca\n",
    "ax4 = axes[1, 1]\n",
    "times = [float(t) for t in df_results['Time_per_Epoch']]\n",
    "ax4.bar(x_pos, times, color='#1abc9c', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Experimento', fontweight='bold', fontsize=11)\n",
    "ax4.set_ylabel('Tiempo (segundos)', fontweight='bold', fontsize=11)\n",
    "ax4.set_title('Tiempo de Entrenamiento por √âpoca', fontweight='bold', fontsize=13)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels([f\"Exp {i}\" for i in x])\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for i, v in enumerate(times):\n",
    "    ax4.text(i, v + max(times)*0.02, f'{v:.2f}s', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c882cb7",
   "metadata": {},
   "source": [
    "### Resumen de T√©cnicas Implementadas\n",
    "\n",
    "**‚úÖ T√©cnicas Obligatorias:**\n",
    "- **Batch Normalization**: Implementado en CNN2 y CNN3\n",
    "- **Data Augmentation**: Aplicado en entrenamiento (rotation, shift, flip, zoom, shear)\n",
    "\n",
    "**‚úÖ T√©cnicas Opcionales (Puntos Extra):**\n",
    "- **Dropout**: Implementado con diferentes valores (0.3 en CNN2, progresivo 0.2‚Üí0.5 en CNN3)\n",
    "- **Early Stopping**: Implementado en CNN3 (patience=5)\n",
    "- **Learning Rate Scheduling**: Implementado en CNN3 (ReduceLROnPlateau)\n",
    "\n",
    "**üìù Transformaciones de Data Augmentation aplicadas:**\n",
    "- Rotation: ¬±20¬∞\n",
    "- Width/Height Shift: ¬±20%\n",
    "- Horizontal Flip: S√≠\n",
    "- Zoom: ¬±15%\n",
    "- Shear: ¬±15%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f492b2e",
   "metadata": {},
   "source": [
    "### Conclusiones del An√°lisis de Arquitecturas\n",
    "\n",
    "**An√°lisis comparativo:**\n",
    "\n",
    "1. **Baseline MLP vs CNNs**: Las arquitecturas CNN superan significativamente al baseline MLP, demostrando la importancia de las capas convolucionales para extracci√≥n de caracter√≠sticas en im√°genes.\n",
    "\n",
    "2. **Impacto de Batch Normalization**: La incorporaci√≥n de BN en CNN2 mejor√≥ la estabilidad del entrenamiento y redujo el overfitting comparado con CNN1.\n",
    "\n",
    "3. **Dropout y Regularizaci√≥n**: El uso de dropout progresivo en CNN3 ayud√≥ a prevenir overfitting, especialmente en las capas m√°s profundas.\n",
    "\n",
    "4. **Early Stopping y LR Scheduling**: Estas t√©cnicas permitieron optimizar el tiempo de entrenamiento y mejorar la convergencia del modelo.\n",
    "\n",
    "5. **Trade-off Complejidad vs Performance**: CNN3 tiene m√°s par√°metros pero mejor generalizaci√≥n, mientras que CNN1 es m√°s ligera pero con mayor overfitting.\n",
    "\n",
    "**Mejor modelo**: [El modelo con mejor balance entre accuracy de test y overfitting ser√° identificado despu√©s del entrenamiento]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
